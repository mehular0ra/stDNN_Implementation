{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/ssd1/mehul_data/research2/ABIDE_fc_data.npy'\n",
    "data = np.load(path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['corr', 'label', 'site', 'age', 'sex'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{116: 119, 196: 128, 236: 85, 78: 25, 176: 210, 296: 119, 246: 56, 146: 59, 152: 28, 206: 28, 124: 4, 316: 3, 232: 1, 202: 1}\n"
     ]
    }
   ],
   "source": [
    "# type(data['corr'])\n",
    "lengths_dict = dict()\n",
    "for sub in data['corr']:\n",
    "    # lengths_set.add(len(sub[0]))\n",
    "    lengths_dict[sub[0].shape[0]] = lengths_dict.get(sub[0].shape[0], 0) + 1\n",
    "print(lengths_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stDNN(nn.Module):\n",
    "    def __init__(self, num_channels=246, num_classes=2):\n",
    "        super(stDNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_channels, out_channels=256, kernel_size=7)\n",
    "        self.conv2 = nn.Conv1d(in_channels=256, out_channels=256, kernel_size=7)\n",
    "        self.conv3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=5)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=4, stride=2)\n",
    "        self.fc = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First 1D CNN block\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # Second 1D CNN block\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        # Temporal averaging\n",
    "        x = torch.mean(x, 2)\n",
    "        # Classification layer\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Feature batch shape: torch.Size([32, 246, 1000])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class fMRIDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input features with shape (num_samples, num_channels, sequence_length).\n",
    "            y (Tensor): Labels with shape (num_samples,).\n",
    "        \"\"\"\n",
    "        assert x.size(0) == y.size(0), \"The number of samples in x and y should be equal.\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the sample to return.\n",
    "        Returns:\n",
    "            tuple: (feature, label) where feature is the input data at index `idx` and label is its corresponding label.\n",
    "        \"\"\"\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# Create synthetic data\n",
    "num_samples = 1000\n",
    "num_channels = 246\n",
    "num_classes = 2\n",
    "sequence_length = 1000  # Assuming each sample is a time series with 1000 time points\n",
    "\n",
    "# Synthetic features\n",
    "x = torch.randn(num_samples, num_channels, sequence_length)\n",
    "# Synthetic labels: Generate random binary labels\n",
    "y = torch.randint(0, num_classes, (num_samples,))\n",
    "\n",
    "# Instantiate the dataset\n",
    "synthetic_dataset = fMRIDataset(x, y)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(synthetic_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Example of iterating over the DataLoader\n",
    "for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(f\"Feature batch shape: {features.size()}\")\n",
    "    print(f\"Labels batch shape: {labels.size()}\")\n",
    "    # Here, you can feed 'features' and 'labels' to your model for training\n",
    "    break  # Breaking here just to demonstrate; remove this in real training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def create_data_loaders(dataset, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, batch_size=32):\n",
    "    assert train_ratio + val_ratio + test_ratio == 1, \"Ratios must sum to 1\"\n",
    "    \n",
    "    total_size = len(dataset)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    test_size = total_size - train_size - val_size  # Ensures all samples are used\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # Shuffle=False for test set\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Using the function to create DataLoader instances for train, val, and test\n",
    "train_loader, val_loader, test_loader = create_data_loaders(synthetic_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f80ee2100d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "class stDNNTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader=None, batch_size=32, learning_rate=0.0001, save_path='best_model.pth'):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.save_path = save_path  # Path to save the best model\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        for inputs, labels in self.train_loader:\n",
    "            inputs, labels = inputs.float(), labels.float()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            probs = torch.sigmoid(outputs.squeeze())\n",
    "            preds = torch.round(probs)\n",
    "            print(preds)\n",
    "            print(labels)\n",
    "            break\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        accuracy, precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        try:\n",
    "            auc = roc_auc_score(all_labels, all_probs)\n",
    "        except ValueError:\n",
    "            auc = np.nan  # Handle cases where AUC can't be computed\n",
    "        \n",
    "        return avg_loss, accuracy, precision, recall, f1, auc\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, accuracy, precision, recall, f1, auc = self.train_epoch()\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train - Loss: {train_loss:.4f}, Acc: {accuracy:.4f}, Prec: {precision:.4f}, Rec: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "            \n",
    "            if self.val_loader:\n",
    "                val_metrics = self.validate()\n",
    "                print(f\"Validation - Loss: {val_metrics['val_loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, Prec: {val_metrics['precision']:.4f}, Rec: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}, AUC: {val_metrics['auc']:.4f}\")\n",
    "                print()\n",
    "                # Save the best model based on validation loss\n",
    "                if val_metrics['val_loss'] < self.best_val_loss:\n",
    "                    self.best_val_loss = val_metrics['val_loss']\n",
    "                    torch.save(self.model.state_dict(), self.save_path)\n",
    "                    print(f\"Saved new best model at epoch {epoch+1}\")\n",
    "\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.val_loader:\n",
    "                inputs, labels = inputs.float(), labels.float()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs.squeeze(), labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                probs = torch.sigmoid(outputs.squeeze())\n",
    "                preds = torch.round(probs)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(self.val_loader)\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        # Handle AUC calculation safely\n",
    "        try:\n",
    "            auc = roc_auc_score(all_labels, all_probs)\n",
    "        except ValueError:\n",
    "            auc = np.nan  # Assign NaN if AUC can't be computed\n",
    "        \n",
    "        metrics = {\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"auc\": auc\n",
    "        }\n",
    "        \n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validate_model(model_class, dataset, num_splits=5, batch_size=32, epochs=15, learning_rate=0.0001):\n",
    "    kf = KFold(n_splits=num_splits, shuffle=True)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset)))):\n",
    "        print(f\"Fold {fold + 1}/{num_splits}\")\n",
    "\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "        val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_subsampler)\n",
    "        \n",
    "        model = model_class(num_channels=246, num_classes=1)  # Ensure this matches your model's expected input\n",
    "        trainer = stDNNTrainer(model, train_loader, val_loader, batch_size, learning_rate)\n",
    "        \n",
    "        trainer.train(epochs)\n",
    "        metrics = trainer.validate()  # Collect metrics from validation\n",
    "        \n",
    "        print(f\"Fold {fold+1} Metrics: {metrics}\")  # Optionally print out fold metrics here for immediate feedback\n",
    "        \n",
    "        fold_results.append(metrics)\n",
    "        \n",
    "    return fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/2\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       grad_fn=<RoundBackward0>)\n",
      "tensor([1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehul/.conda/envs/pyg/lib/python3.11/site-packages/torch/autograd/__init__.py:200: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/mehul/.conda/envs/pyg/lib/python3.11/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/mehul/.conda/envs/pyg/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m synthetic_dataset \u001b[38;5;241m=\u001b[39m fMRIDataset(x, y)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mcross_validate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstDNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthetic_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m, in \u001b[0;36mcross_validate_model\u001b[0;34m(model_class, dataset, num_splits, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(num_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m246\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Ensure this matches your model's expected input\u001b[39;00m\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m stDNNTrainer(model, train_loader, val_loader, batch_size, learning_rate)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mvalidate()  \u001b[38;5;66;03m# Collect metrics from validation\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Metrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Optionally print out fold metrics here for immediate feedback\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 55\u001b[0m, in \u001b[0;36mstDNNTrainer.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 55\u001b[0m         train_loss, accuracy, precision, recall, f1, auc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Prec: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Rec: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loader:\n",
      "Cell \u001b[0;32mIn[16], line 45\u001b[0m, in \u001b[0;36mstDNNTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m     all_probs\u001b[38;5;241m.\u001b[39mextend(probs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     44\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)\n\u001b[0;32m---> 45\u001b[0m accuracy, precision, recall, f1, _ \u001b[38;5;241m=\u001b[39m precision_recall_fscore_support(all_labels, all_preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     auc \u001b[38;5;241m=\u001b[39m roc_auc_score(all_labels, all_probs)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "# Create synthetic data\n",
    "num_samples = 1000\n",
    "num_channels = 246\n",
    "num_classes = 2\n",
    "sequence_length = 1000  # Assuming each sample is a time series with 1000 time points\n",
    "\n",
    "# Synthetic features\n",
    "x = torch.randn(num_samples, num_channels, sequence_length)\n",
    "# Synthetic labels: Generate random binary labels\n",
    "y = torch.randint(0, num_classes, (num_samples,))\n",
    "\n",
    "# Instantiate the dataset\n",
    "synthetic_dataset = fMRIDataset(x, y)\n",
    "\n",
    "cross_validate_model(stDNN, synthetic_dataset, num_splits=2, batch_size=32, epochs=15, learning_rate=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
